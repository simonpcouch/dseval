---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# DSEval

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/DSBench)](https://CRAN.R-project.org/package=DSBench)
<!-- badges: end -->

This repository implements an adaptation of [DSBench](https://arxiv.org/abs/2409.07703), a data science LLM evaluation, in R using [vitals](https://vitals.tidyverse.org/).

The original benchmark contains 466 data analysis questions:

* In DSBench, each question has a multiple choice answer. This is an issue: very seldomly in real life do data science tasks end with a multiple choice answer. Also, this increases the risk of sandbagging, where models will realize theyâ€™re being evaluated and their behavior might change as a result. DSEval rephrases questions to have open-ended answers.
* In DSBench, the .xslx, .csv, and other source files are inlined into the user prompt. In DSEval, models are situated in a working directory where those files exist.
* File names and references are renamed to be less indicative of competition data science / model evaluations, also to descrease the risk of sandbagging.
* Rather than requiring the agent to complete the task in one fell swoop, the agent is allowed to ask a user questions. That "user" is actually a "model-in-the-middle", instructed to generally provide a thumbs-up and little guidance.

> IMPORTANT
>
> This evaluation is highly experimental and much of its documentation is aspirational.

# Example

```{r}
dseval_dataset
```

```{r}
#| eval: false
tsk <- dseval_task()
tsk

tsk$eval(solver_chat = databot:::chat_bot())
```
